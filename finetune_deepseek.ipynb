{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nzdor\\anaconda3\\envs\\MachineLearning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import urllib.request\n",
    "import wandb\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    SchedulerType,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the wandb API key from file\n",
    "with open(\"wandb_api_key.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "# Log in\n",
    "wandb.login(key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Weights & Biases with custom settings\n",
    "wandb.init(\n",
    "    project=\"deepseek-finetuning\",      # Organise runs under this project\n",
    "    entity=\"your_wandb_username\",       # Replace with your wandb username or team name\n",
    "    name=\"run_cosine_scheduler\",        # A custom name for this run\n",
    "    config={                           # Hyperparameters and settings\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"batch_size\": 2,\n",
    "        \"max_length\": 2048,\n",
    "        \"lr_scheduler\": \"cosine\",\n",
    "        \"warmup_steps\": 10\n",
    "    },\n",
    "    tags=[\"finetuning\", \"DeepSeek\", \"cosine\", \"experiment\"],\n",
    "    notes=\"Using cosine annealing with wandb integration for detailed tracking.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [5e-5, 3e-5, 1e-4]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [2, 4]\n",
    "        },\n",
    "        'warmup_steps': {\n",
    "            'values': [10, 20]\n",
    "        },\n",
    "        'num_train_epochs': {\n",
    "            'values': [3, 4]\n",
    "        },\n",
    "        'max_length': {\n",
    "            'values': [1024, 2048]  # varying this to see if it's important to capture later details of papers, or if we can get away with less\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset that streams articles on the fly\n",
    "class PMCDataset(Dataset):\n",
    "    def __init__(self, file_list, base_url, tokenizer, max_length=512):\n",
    "        self.file_paths = file_list\n",
    "        self.base_url = base_url\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.file_paths[idx]\n",
    "        url = self.base_url + path\n",
    "        try:\n",
    "            response = urllib.request.urlopen(url)\n",
    "            article_bytes = response.read()\n",
    "            article_text = article_bytes.decode('utf-8', errors='ignore')\n",
    "        except Exception as e:\n",
    "            article_text = \"\"\n",
    "        tokenised = self.tokenizer(\n",
    "            article_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        tokenised = {key: value.squeeze(0) for key, value in tokenised.items()}\n",
    "        return tokenised\n",
    "\n",
    "# Callback that collects loss values and plots them at the end of training.\n",
    "class LossPlottingCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.steps = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None and \"loss\" in logs:\n",
    "            self.steps.append(state.global_step)\n",
    "            self.losses.append(logs[\"loss\"])\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.steps, self.losses, label=\"Training Loss\")\n",
    "        plt.xlabel(\"Global Step\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training Loss Across Steps\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go\n",
    "\n",
    "def main():\n",
    "    # Initialise wandb. When running under a sweep agent, wandb.init() loads hyperparameters into wandb.config.\n",
    "    wandb.init()\n",
    "    \n",
    "    # Access hyperparameters via wandb.config. Defaults are provided if not running in sweep mode.\n",
    "    config = wandb.config\n",
    "    num_train_epochs = getattr(config, \"num_train_epochs\", 3)\n",
    "    batch_size = getattr(config, \"batch_size\", 2)\n",
    "    learning_rate = getattr(config, \"learning_rate\", 5e-5)\n",
    "    warmup_steps = getattr(config, \"warmup_steps\", 10)\n",
    "    max_length = getattr(config, \"max_length\", 2048)\n",
    "    \n",
    "    # Configurable parameters for dataset\n",
    "    FILE_LIST = \"oa_file_list.txt\"\n",
    "    BASE_URL = \"ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/\"\n",
    "    NUMBER_OF_ARTICLES = 10  # For proof-of-concept\n",
    "    \n",
    "    # Read and sample the file list.\n",
    "    with open(FILE_LIST, \"r\", encoding=\"utf-8\") as f:\n",
    "        file_paths = [line.split('\\t')[0] for line in f if line.strip()]\n",
    "    if len(file_paths) > NUMBER_OF_ARTICLES:\n",
    "        file_paths = random.sample(file_paths, NUMBER_OF_ARTICLES)\n",
    "    \n",
    "    # Load tokenizer and model from Hugging Face.\n",
    "    model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Create the streaming dataset.\n",
    "    dataset = PMCDataset(file_paths, BASE_URL, tokenizer, max_length=max_length)\n",
    "    \n",
    "    # Define training arguments with cosine annealing and wandb integration.\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./finetuned_model\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        logging_steps=5,\n",
    "        save_steps=50,\n",
    "        evaluation_strategy=\"no\",\n",
    "        fp16=True,\n",
    "        lr_scheduler_type=SchedulerType.COSINE,\n",
    "        warmup_steps=warmup_steps,\n",
    "        report_to=[\"wandb\"],\n",
    "        disable_tqdm=False,  # Ensure tqdm progress bar is enabled\n",
    "    )\n",
    "    \n",
    "    # Create the Trainer instance with our loss plotting callback.\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        callbacks=[LossPlottingCallback()],\n",
    "    )\n",
    "    \n",
    "    # Start fine-tuning.\n",
    "    trainer.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a new sweep every time the script is run.\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"deepseek-finetuning\")\n",
    "    print(\"Sweep ID:\", sweep_id)\n",
    "    # Launch the sweep agent; this will call the main() function with different hyperparameters.\n",
    "    wandb.agent(sweep_id, function=main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
