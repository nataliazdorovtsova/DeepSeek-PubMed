{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import urllib.request\n",
    "import wandb\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    SchedulerType,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the wandb API key from file\n",
    "with open(\"wandb_api_key.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "# Log in\n",
    "wandb.login(key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [3e-5, 1e-4]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [2]\n",
    "        },\n",
    "        'warmup_steps': {\n",
    "            'values': [20]\n",
    "        },\n",
    "        'num_train_epochs': {\n",
    "            'values': [1]\n",
    "        },\n",
    "        'max_length': {\n",
    "            'values': [1024, 2048]  # varying this to see if it's important to capture later details of papers, or if we can get away with less\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset that streams articles on the fly\n",
    "class PMCDataset(Dataset):\n",
    "    def __init__(self, file_list, base_url, tokenizer, max_length=512):\n",
    "        self.file_paths = file_list\n",
    "        self.base_url = base_url\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.file_paths[idx]\n",
    "        url = self.base_url + path\n",
    "        try:\n",
    "            response = urllib.request.urlopen(url)\n",
    "            article_bytes = response.read()\n",
    "            article_text = article_bytes.decode('utf-8', errors='ignore')\n",
    "        except Exception as e:\n",
    "            article_text = \"\"\n",
    "        tokenised = self.tokenizer(\n",
    "            article_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        tokenised = {key: value.squeeze(0) for key, value in tokenised.items()}\n",
    "        return tokenised\n",
    "\n",
    "# Callback that collects loss values and plots them at the end of training.\n",
    "class LossPlottingCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.steps = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None and \"loss\" in logs:\n",
    "            self.steps.append(state.global_step)\n",
    "            self.losses.append(logs[\"loss\"])\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.steps, self.losses, label=\"Training Loss\")\n",
    "        plt.xlabel(\"Global Step\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training Loss Across Steps\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go\n",
    "\n",
    "def main():\n",
    "    # Initialise wandb without a fixed name.\n",
    "    wandb.init(\n",
    "        project=\"deepseek-finetuning\",\n",
    "        entity=\"nzdorovtsova\",\n",
    "        tags=[\"finetuning\", \"DeepSeek\", \"cosine\", \"experiment\"],\n",
    "        notes=\"Finetuning DeepSeek-R1 1.5B on PMC open access articles.\",\n",
    "        settings=wandb.Settings(init_timeout=180)\n",
    "    )\n",
    "\n",
    "    # Update run name dynamically based on the hyperparameters provided by the sweep.\n",
    "    config = wandb.config\n",
    "    wandb.run.name = f\"lr{config.learning_rate}_epochs{config.num_train_epochs}_bs{config.batch_size}_maxlen{config.max_length}\"\n",
    "    \n",
    "    # Retrieve hyperparameters directly from wandb.config (provided by the sweep agent)\n",
    "    config = wandb.config\n",
    "    num_train_epochs = config.num_train_epochs\n",
    "    batch_size = config.batch_size\n",
    "    learning_rate = config.learning_rate\n",
    "    warmup_steps = config.warmup_steps\n",
    "    max_length = config.max_length\n",
    "    \n",
    "    # Configurable parameters for dataset\n",
    "    FILE_LIST = \"oa_file_list.txt\"\n",
    "    BASE_URL = \"ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/\"\n",
    "    NUMBER_OF_ARTICLES = 10  # For proof-of-concept\n",
    "    \n",
    "    # Read and sample the file list.\n",
    "    with open(FILE_LIST, \"r\", encoding=\"utf-8\") as f:\n",
    "        file_paths = [line.split('\\t')[0] for line in f if line.strip()]\n",
    "    if len(file_paths) > NUMBER_OF_ARTICLES:\n",
    "        file_paths = random.sample(file_paths, NUMBER_OF_ARTICLES)\n",
    "    \n",
    "    # Load tokenizer and model from Hugging Face.\n",
    "    model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Create the streaming dataset.\n",
    "    dataset = PMCDataset(file_paths, BASE_URL, tokenizer, max_length=max_length)\n",
    "    \n",
    "    # Define training arguments with cosine annealing and wandb integration.\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./finetuned_model\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        logging_steps=5,\n",
    "        save_steps=50,\n",
    "        evaluation_strategy=\"no\",\n",
    "        fp16=True,  # Use mixed precision if supported\n",
    "        lr_scheduler_type=SchedulerType.COSINE,\n",
    "        warmup_steps=warmup_steps,\n",
    "        report_to=[\"wandb\"],\n",
    "        disable_tqdm=False  # Ensure tqdm progress is visible in VSCode's terminal.\n",
    "    )\n",
    "    \n",
    "    # Create the Trainer instance with our loss plotting callback.\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        callbacks=[LossPlottingCallback()],\n",
    "    )\n",
    "    \n",
    "    # Start fine-tuning.\n",
    "    trainer.train()\n",
    "\n",
    "# Main entry point - for now we are always running as a sweep agent.\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a new sweep and immediately launch the sweep agent.\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"deepseek-finetuning\")\n",
    "    print(\"Sweep ID:\", sweep_id)\n",
    "    wandb.agent(sweep_id, function=main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
